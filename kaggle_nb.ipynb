{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\n\nclass Linear:\n    \"\"\"\n    A linear (fully connected) layer of a neural network.\n\n    Attributes:\n    \n    weights : np.ndarray\n        Weight matrix of shape (n_neurons, n_inputs).\n    biases : np.ndarray\n        Bias vector of shape (n_neurons, 1).\n    input : np.ndarray\n        Input data of shape (n_inputs, m).\n    output : np.ndarray\n        Output data of shape (n_neurons, m).\n    m : int\n        Number of examples (columns) in the input data.\n    \"\"\"\n\n    def __init__(self,n_inputs,n_neurons):\n        \"\"\"\n        Initializes linear layer with random weights and zero biases\n        \n        Args:\n        n_inputs : int\n            number of input features\n         n_neurons : int\n            number of neurons in the layer\n        \"\"\"\n\n\n        self.weights=np.random.randn(n_neurons,n_inputs)*0.10\n        self.biases=np.zeros((n_neurons,1))\n        \n\n    def forward(self,X):\n        \"\"\"\n        Performs forward pass of the layer\n\n        Args:\n\n        X : np.ndarray\n            Input of the layer of shape (n_inputs,m)\n\n        Returns:\n        np.ndarray\n           Output of the layer of shape (n_neurons,m)\n\n        \n        \"\"\"\n        self.input=X\n        self.m=X.shape[1]\n        self.output=np.dot(self.weights,X) + self.biases\n        return self.output \n    \n    def backward(self,dz):\n        \"\"\"\n        Performs backward pass of the layer\n       \n        Args:\n\n        dz : np.ndarray\n            Gradient of loss wrt output of the layer\n\n        Returns: \n        np.ndarray\n            Gradient of loss wrt input of the layer\n        \"\"\"\n        self.dweights=np.dot(dz,self.input.T)/self.m\n        self.dbiases=np.sum(dz,axis=1,keepdims=True)/self.m\n        return np.dot(self.weights.T,dz)\n        \n\nclass ReLU:\n    \"\"\"\n    Rectified Linear Unit(ReLU) activation function\n    \"\"\"\n    def forward(self,X):\n        \"\"\"\n        Performs the forward pass\n\n        Args:\n\n        X : np.ndarray \n           Input data \n\n        Returns :\n        np.ndarray\n            Output data after applying ReLU          \n        \n        \"\"\"\n        self.input=X\n        return np.maximum(0,X)\n \n    def backward(self,gradient):\n        \"\"\"\n        Performs the backward pass\n\n        Args:\n        gradient : np.ndarray\n                Gradient of loss wrt output\n       \n        Returns :\n        np.ndarray\n                Gradient of loss wrt input\n              \n        \"\"\"\n        return gradient*(self.input>0)\n\n\nclass Sigmoid:\n    \"\"\"\n    Sigmoid Activation Function\n    \"\"\"\n    def forward(self,X):\n        \"\"\"\n        Performs the forward pass\n\n        Args:\n\n        X : np.ndarray \n           Input data \n\n        Returns :\n        np.ndarray\n            Output data after applying Sigmnoid   \n        \n        \"\"\"\n        self.input=X\n        self.output=1/(1+np.exp(-X))\n        return self.output\n    \n    def backward(self,gradient):\n        \n        \"\"\"\n        Performs the backward pass\n\n        Args:\n        gradient : np.ndarray\n                Gradient of loss wrt output\n       \n        Returns :\n        np.ndarray\n                Gradient of loss wrt input\n             \n        \"\"\"\n        return self.output*(1-self.output)*gradient\n    \n\nclass Tanh:\n    \"\"\"\n    Hyperbolic tangent (tanh) activation function.\n    \"\"\"\n    def forward(self,X):\n        \"\"\"\n        Performs the forward pass\n\n        Args:\n\n        X : np.ndarray \n           Input data \n\n        Returns :\n        np.ndarray\n            Output data after applying Tanh   \n        \n        \"\"\"\n        self.input=X\n        self.output=np.tanh(X)\n        return self.output\n    \n    def backward(self,gradient):\n        \"\"\"\n        Performs the backward pass\n\n        Args:\n        gradient : np.ndarray\n                Gradient of loss wrt output\n       \n        Returns :\n        np.ndarray\n                Gradient of loss wrt input\n             \n        \"\"\"\n        return gradient*(1-self.output**2)\n    \nclass Softmax:\n    \"\"\"\n    Softmax Activation Function\n    \"\"\"\n    def forward(self,X):\n        \"\"\"\n        Performs the forward pass\n\n        Args:\n        X : np.ndarray\n            Input data\n\n        Returns:\n        np.ndarray\n            Output data after applying softmax\n\n        \"\"\"\n        self.input=X\n        X_norm = X - np.max(X, axis=0, keepdims=True)\n        exp_values = np.exp(X_norm)\n        self.output = exp_values / np.sum(exp_values, axis=0, keepdims=True)\n        return self.output \n\n    def backward(self,gradient):\n        \"\"\"\n        Performs backward pass\n\n        Args:\n        gradient : np.ndaray\n            Gradient of loss wrt output\n\n        Returns:\n        np.ndarray\n            Gradient of loss wrt input\n        \"\"\"\n        return gradient\n\nclass CrossEntropyLoss:\n    \"\"\"\n    Cross-Entropy loss function\n    \"\"\"\n\n\n    def one_hot(self,y_true):\n            \"\"\"\n            Converts integer labels to one-hot encoded labels\n\n            Args:\n            y_true : np.ndarray\n                   Array of true labels\n            \n            Returns:\n            np.ndarray\n                   One-hot encoded label matrix\n            \"\"\"\n            num_cat=np.max(y_true)+1\n            b=np.zeros((num_cat,y_true.shape[1]))\n            b[y_true,np.arange(y_true.shape[1])]=1\n            return b\n\n        \n        \n    def forward(self,y_pred,y_true):\n        \"\"\"\n        Computes forward pass of the loss\n        \n        Args:\n        y_pred : np.ndarray\n               Predicted probabilities\n        \n        y_true : np.ndarray\n                True labels\n\n        Returns: \n        float \n           Computed loss value\n        \n        \"\"\"\n        m=y_true.shape[1]\n        self.y_pred=np.clip(y_pred,1e-7,1-(1e-7))\n     \n        self.y_true=self.one_hot(y_true)\n      \n        self.loss=-np.sum(self.y_true*np.log(self.y_pred))\n        self.loss=self.loss/m\n        return self.loss\n    \n    def backward(self):\n        \"\"\"\n        Computes backward pass of the loss\n\n        Returns:\n        np.ndarray \n             Gradient of loss wrt predicted output\n\n        \"\"\"\n     \n        return (self.y_pred-self.y_true)/self.y_pred.shape[1]\n\n\nclass MSE:\n    \"\"\"\n    Mean Sqaured Error (MSE) loss function\n    \"\"\"\n\n    def forward(self,y_pred,y_true):\n        \"\"\"\n        Computes forward pass of the loss\n\n        Args:\n        y_pred : np.ndarray\n               Predicted values\n        \n        y_true : np.ndarray\n                True values\n        \n        Returns: \n        float \n           Computed loss value\n\n        \"\"\"\n        self.y_pred=y_pred\n        self.y_true=y_true\n        self.diff=y_pred-y_true\n\n        return np.sum(np.square(self.diff))/y_pred.shape[1]\n    \n    def backward(self):\n        \"\"\"\n        Computes backward pass of the loss\n\n        Returns:\n        np.ndarray \n             Gradient of loss wrt predicted output\n\n        \"\"\"\n        return 2 * (self.y_pred - self.y_true) / self.y_true.shape[1]\n\n\nclass SGD:\n    \"\"\"\n    Stochastic Gradient Desecent (SGD) Fucntion \n    \"\"\"\n    def __init__ (self,learning_rate):\n        \"\"\"\n        Initializes the optimizer with a learning rate\n        \"\"\"\n        self.learning_rate=learning_rate\n\n    def step(self,layers):\n        \"\"\"\n        Updates the weights and biases of the layers of the model\n\n        Args:\n        layers : list\n             List of layers in the model\n        \"\"\"\n        for layer in layers:\n            if isinstance(layer, Linear):\n                layer.weights -= self.learning_rate * layer.dweights\n                layer.biases -= self.learning_rate * layer.dbiases\n\nclass Model:\n    \"\"\"\n    A neural network model that allows for adding layers, compiling, training, evaluating, saving and loading.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initializes the model with an empty list of layers, loss function and an optimizer        \n        \"\"\"\n        self.layers=[]\n        self.loss=None\n        self.optimizer=None\n\n    def add_layer(self,layer):\n        \"\"\"\n        Adds a layer to the model\n\n        Args:\n        Layer : obejct\n              A layer (Linear, ReLu, etc.) to add to the model\n        \"\"\"\n        self.layers.append(layer)\n\n    def compile(self,loss,optimizer):\n        \"\"\"\n        Compiles the model with a loss function and an optimizer\n\n        Args:\n        loss : object\n             Loss function to use\n        \n        optimizer : object\n             Optimizer to use\n        \"\"\"\n        self.loss=loss\n        self.optimizer=optimizer\n\n    def forward(self,X):\n        \"\"\"\n        Performs forward pass through all the layers\n\n        Args:\n        X : np.ndarray\n            Input data\n        \n        Returns:\n        output : np.ndarray\n             Output of the final layer\n        \"\"\"\n        output=X\n        for layer in self.layers:\n            output=layer.forward(output)\n        return output\n    \n    def backward(self):\n        \"\"\"\n        Performs backward pass through all the layers\n        \"\"\"\n        gradient=self.loss.backward()\n        for layer in reversed(self.layers):\n            gradient=layer.backward(gradient)\n    \n    def train(self,x_train,y_train,epochs,batch_size):\n        \"\"\"\n        Trains the model\n         \n        Args:\n        x_train : np.ndarray\n               Training data\n\n        y_train : np.ndarray\n               Training labels\n        \n        epochs : int\n               Number of epochs to train\n        \n        batch_size : int\n               Batch size to use for training\n        \n               \n        \"\"\"\n        \n        for epoch in range(epochs):\n           m=x_train.shape[1]\n           \n           for i in range(0,m,batch_size):\n               x_batch=x_train[:,i:i+batch_size]\n               y_batch=y_train[:,i:i+batch_size]\n\n               \n               y_pred=self.forward(x_batch)\n               loss=self.loss.forward(y_pred,y_batch)\n               self.backward()\n               self.optimizer.step(self.layers)\n              \n       \n\n\n    def predict(self,x_test):\n        \"\"\"\n        Predicts the labels for test data\n\n        Args:\n        x_test : np.ndarray\n               Test data\n        \n        Returns : np.ndarray\n               Predicted labels\n        \"\"\"\n        y_pred=self.forward(x_test)\n        y_pred=np.argmax(y_pred,axis=0)\n        return y_pred\n\n\n\n    def evaluate(self,x_test,y_test):\n        \"\"\"\n        Evaluates the performace of the model on test data\n\n        Args:\n        x_test : np.ndarray\n               Test data\n       \n        y_test : np.ndarray\n                Predicted Labels\n        \n        Returns :\n        tuple :\n               Loss and accuracy of model on test data     \n\n\n        \"\"\"\n        y_pred=self.forward(x_test)\n        loss=self.loss.forward(y_pred,y_test)\n        y_pred=np.argmax(y_pred,axis=0)\n        accuracy=100*np.sum((y_pred==y_test.flatten()))/y_test.shape[1]\n        return loss,accuracy\n    \n    def save(self):\n        \"\"\"\n        Saves the model parameters\n\n        Returns:\n        tuple\n            The layers, loss, and optimizer of the model\n        \"\"\"\n        return self.layers,self.loss,self.optimizer\n    \n    def load(self,saved_layers,saved_loss,saved_optimizer):\n        \"\"\"\n        Loads the model parameters.\n\n        Parameters:\n        -----------\n        saved_layers : list\n                   Layers to load\n\n        saved_loss : object\n                   Loss function to load\n\n        saved_optimizer : object\n                   Optimizer to load\n        \n        \"\"\"\n        self.layers=saved_layers\n        self.loss=saved_loss\n        self.optimizer=saved_optimizer\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-10T10:07:25.137681Z","iopub.execute_input":"2024-08-10T10:07:25.138096Z","iopub.status.idle":"2024-08-10T10:07:25.210508Z","shell.execute_reply.started":"2024-08-10T10:07:25.138052Z","shell.execute_reply":"2024-08-10T10:07:25.209407Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}